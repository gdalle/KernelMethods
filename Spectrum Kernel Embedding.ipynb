{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T11:58:57.846700Z",
     "start_time": "2019-02-13T11:58:56.876613Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T11:58:57.873845Z",
     "start_time": "2019-02-13T11:58:57.850955Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('kernel-methods-for-machine-learning-2018-2019/Xtr0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T11:58:57.892666Z",
     "start_time": "2019-02-13T11:58:57.876106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>GGAGAATCATTTGAACCCGGGAGGTGGAGGTTGCCGTGAGCTGAGA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ACCCTGCCTACACCGCGGCGGGGACAGGTGGAGGTTTCAACCCCTG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>TGCAAATCTGTAAGCATTTCTCAGGCAATGAATTATGTCAACACAA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>GCGGGACGTGGGCGTCGAGGGTAAGGATATCTGCAGAAGTACTGTC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>GGAGAATAGCATGTATCCGAGAGGTGGAGCTGGCAGTGAGCCGAGA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                                seq\n",
       "0   0  GGAGAATCATTTGAACCCGGGAGGTGGAGGTTGCCGTGAGCTGAGA...\n",
       "1   1  ACCCTGCCTACACCGCGGCGGGGACAGGTGGAGGTTTCAACCCCTG...\n",
       "2   2  TGCAAATCTGTAAGCATTTCTCAGGCAATGAATTATGTCAACACAA...\n",
       "3   3  GCGGGACGTGGGCGTCGAGGGTAAGGATATCTGCAGAAGTACTGTC...\n",
       "4   4  GGAGAATAGCATGTATCCGAGAGGTGGAGCTGGCAGTGAGCCGAGA..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All sequences is this set have length 101."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T11:58:57.901544Z",
     "start_time": "2019-02-13T11:58:57.895814Z"
    }
   },
   "outputs": [],
   "source": [
    "letters = 'ATCG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T11:58:57.907090Z",
     "start_time": "2019-02-13T11:58:57.904370Z"
    }
   },
   "outputs": [],
   "source": [
    "length = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T11:58:57.918703Z",
     "start_time": "2019-02-13T11:58:57.910790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AAA': 0, 'AAT': 1, 'AAC': 2, 'AAG': 3, 'ATA': 4, 'ATT': 5, 'ATC': 6, 'ATG': 7, 'ACA': 8, 'ACT': 9, 'ACC': 10, 'ACG': 11, 'AGA': 12, 'AGT': 13, 'AGC': 14, 'AGG': 15, 'TAA': 16, 'TAT': 17, 'TAC': 18, 'TAG': 19, 'TTA': 20, 'TTT': 21, 'TTC': 22, 'TTG': 23, 'TCA': 24, 'TCT': 25, 'TCC': 26, 'TCG': 27, 'TGA': 28, 'TGT': 29, 'TGC': 30, 'TGG': 31, 'CAA': 32, 'CAT': 33, 'CAC': 34, 'CAG': 35, 'CTA': 36, 'CTT': 37, 'CTC': 38, 'CTG': 39, 'CCA': 40, 'CCT': 41, 'CCC': 42, 'CCG': 43, 'CGA': 44, 'CGT': 45, 'CGC': 46, 'CGG': 47, 'GAA': 48, 'GAT': 49, 'GAC': 50, 'GAG': 51, 'GTA': 52, 'GTT': 53, 'GTC': 54, 'GTG': 55, 'GCA': 56, 'GCT': 57, 'GCC': 58, 'GCG': 59, 'GGA': 60, 'GGT': 61, 'GGC': 62, 'GGG': 63}\n"
     ]
    }
   ],
   "source": [
    "def build_voc(letters, length):\n",
    "    vocl = [''.join(x) for x in itertools.product(letters, repeat=length)]\n",
    "    voc = {}\n",
    "    i = 0\n",
    "    for v in vocl:\n",
    "        voc[v] = i\n",
    "        i+=1\n",
    "    return voc\n",
    "voc = build_voc(letters, length)\n",
    "print(voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Phi_u(x)$ is the number of occurrences of u in x (without gaps) : *spectrum kernel* (Leslie et al., 2002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T11:58:57.926063Z",
     "start_time": "2019-02-13T11:58:57.921356Z"
    }
   },
   "outputs": [],
   "source": [
    "def substrings(x, length):\n",
    "    n = len(x)\n",
    "    sub = []\n",
    "    assert n>=length, 'seq too small'\n",
    "    for i in range(n-length+1):\n",
    "        curr = x[i:i+length]\n",
    "        sub.append(curr)\n",
    "    return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T11:58:57.933923Z",
     "start_time": "2019-02-13T11:58:57.927943Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GGA', 'GAG', 'AGA', 'GAA', 'AAT', 'ATC', 'TCA', 'CAT', 'ATT', 'TTT', 'TTG', 'TGA', 'GAA', 'AAC', 'ACC', 'CCC', 'CCG', 'CGG', 'GGG', 'GGA', 'GAG', 'AGG', 'GGT', 'GTG', 'TGG', 'GGA', 'GAG', 'AGG', 'GGT', 'GTT', 'TTG', 'TGC', 'GCC', 'CCG', 'CGT', 'GTG', 'TGA', 'GAG', 'AGC', 'GCT', 'CTG', 'TGA', 'GAG', 'AGA', 'GAT', 'ATT', 'TTG', 'TGC', 'GCG', 'CGC', 'GCC', 'CCA', 'CAT', 'ATT', 'TTG', 'TGC', 'GCA', 'CAC', 'ACT', 'CTC', 'TCC', 'CCA', 'CAG', 'AGC', 'GCC', 'CCT', 'CTG', 'TGG', 'GGG', 'GGC', 'GCA', 'CAA', 'AAC', 'ACA', 'CAA', 'AAG', 'AGA', 'GAG', 'AGC', 'GCA', 'CAA', 'AAA', 'AAA', 'AAC', 'ACT', 'CTC', 'TCT', 'CTG', 'TGT', 'GTC', 'TCT', 'CTC', 'TCA', 'CAC', 'ACA', 'CAA', 'AAA', 'AAA', 'AAC']\n"
     ]
    }
   ],
   "source": [
    "x = df['seq'][0]\n",
    "sub = substrings(x, 3)\n",
    "print(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T11:58:57.940830Z",
     "start_time": "2019-02-13T11:58:57.936025Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GGAGAATCATTTGAACCCGGGAGGTGGAGGTTGCCGTGAGCTGAGATTGCGCCATTGCACTCCAGCCTGGGCAACAAGAGCAAAACTCTGTCTCACAAAAC'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T11:58:57.947984Z",
     "start_time": "2019-02-13T11:58:57.943164Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we reduce to get the feature vector. Let $\\Phi_u(x)$ denote the number of occurrences of $u$ in $x$. The\n",
    "$k$-spectrum kernel is $K(x, x'):= \\sum_{u\\in A^k} \\Phi_u(x) \\Phi_u(x')$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It corresponds to a linear kernel over the feature space. So we may store all sequences in the feature space of all length 3 subsequences. The features will be sparse: at most $|x|-k+1$ non zero features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T11:58:57.955308Z",
     "start_time": "2019-02-13T11:58:57.950074Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode(sub, voc):\n",
    "    enc = np.zeros(len(voc))\n",
    "    for s in sub:\n",
    "        i = voc[s]\n",
    "        enc[i] += 1\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T11:58:57.962915Z",
     "start_time": "2019-02-13T11:58:57.959069Z"
    }
   },
   "outputs": [],
   "source": [
    "encoding = encode(sub, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T11:58:57.972813Z",
     "start_time": "2019-02-13T11:58:57.966560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 1. 4. 1. 0. 3. 1. 0. 2. 2. 1. 0. 3. 0. 3. 2. 0. 0. 0. 0. 0. 1. 0. 4.\n",
      " 2. 2. 1. 0. 3. 1. 3. 2. 4. 2. 2. 1. 0. 0. 3. 3. 2. 1. 1. 2. 0. 1. 1. 1.\n",
      " 2. 1. 0. 6. 0. 1. 1. 2. 3. 1. 3. 1. 3. 2. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T11:58:57.980246Z",
     "start_time": "2019-02-13T11:58:57.974763Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the embedded data matrices (exact matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T11:58:57.987788Z",
     "start_time": "2019-02-13T11:58:57.982246Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has size 256\n"
     ]
    }
   ],
   "source": [
    "letters = 'ATCG'\n",
    "length = 4\n",
    "voc = build_voc(letters, length)\n",
    "print('Vocabulary has size', len(voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T11:59:13.915756Z",
     "start_time": "2019-02-13T11:58:57.989981Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:04, 419.80it/s]\n",
      "2000it [00:05, 366.91it/s]\n",
      "2000it [00:04, 424.84it/s]\n"
     ]
    }
   ],
   "source": [
    "for ind in range(3):\n",
    "    df = pd.read_csv('kernel-methods-for-machine-learning-2018-2019/Xtr'+str(ind)+'.csv')\n",
    "    df_emb = pd.DataFrame(columns = [str(i) for i in range(len(voc))])\n",
    "    for _, r in tqdm.tqdm(df.iterrows()):\n",
    "        i = r['Id']\n",
    "        seq = r['seq']\n",
    "        enc = encode(substrings(seq, length), voc)\n",
    "        df_emb.loc[i] = enc\n",
    "    df_emb.to_csv('kernel-methods-for-machine-learning-2018-2019/'\n",
    "              + 'Xtr' +str(ind) + '_spectr'+str(length)+'.csv', header = False, index = False, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T11:59:19.742205Z",
     "start_time": "2019-02-13T11:59:13.917018Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:01, 552.84it/s]\n",
      "1000it [00:01, 592.99it/s]\n",
      "1000it [00:01, 564.60it/s]\n"
     ]
    }
   ],
   "source": [
    "for ind in range(3):\n",
    "    df = pd.read_csv('kernel-methods-for-machine-learning-2018-2019/Xte'+str(ind)+'.csv')\n",
    "    df_emb = pd.DataFrame(columns = [str(i) for i in range(len(voc))])\n",
    "    for _, r in tqdm.tqdm(df.iterrows()):\n",
    "        i = r['Id']\n",
    "        seq = r['seq']\n",
    "        enc = encode(substrings(seq, length), voc)\n",
    "        df_emb.loc[i] = enc\n",
    "    df_emb.to_csv('kernel-methods-for-machine-learning-2018-2019/'\n",
    "              + 'Xte' +str(ind) + '_spectr'+str(length)+'.csv', header = False, index = False, sep=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use suffix tree for mismatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building suffix tree using Ukkonen's algorithm (external lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T11:59:19.753333Z",
     "start_time": "2019-02-13T11:59:19.747424Z"
    }
   },
   "outputs": [],
   "source": [
    "from suffix_trees import STree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-13T11:58:56.912Z"
    }
   },
   "outputs": [],
   "source": [
    "string = 'BANANA' # has 6 suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-13T11:58:56.914Z"
    }
   },
   "outputs": [],
   "source": [
    "st = STree.STree(\"BANANA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-13T11:58:56.916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.find_all('AN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-13T11:58:56.918Z"
    }
   },
   "outputs": [],
   "source": [
    "letters = 'ATCG'\n",
    "length = 3\n",
    "voc = build_voc(letters, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-13T11:58:56.919Z"
    }
   },
   "outputs": [],
   "source": [
    "s0 = df['seq'][0]\n",
    "w = 'AAG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-13T11:58:56.921Z"
    }
   },
   "outputs": [],
   "source": [
    "st0 = STree.STree(s0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build words at 1 Hamming distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-13T11:58:56.922Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_1_neighborhood(w):\n",
    "    nset = []\n",
    "    for i in range(len(w)):\n",
    "        for j in letters:\n",
    "            nset.append(w[:i]+j+w[i+1:])\n",
    "\n",
    "    nset = list(set(nset))\n",
    "    return nset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-13T11:58:56.923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAC', 'AAA', 'AAG', 'CAG', 'AGG', 'GAG', 'ATG', 'ACG', 'TAG', 'AAT']\n"
     ]
    }
   ],
   "source": [
    "nset = build_1_neighborhood(w)\n",
    "print(nset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-13T11:58:56.925Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nset) # 3+3+3 + 1 exact matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-13T11:58:56.927Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(st0.find_all(n)) for n in nset]) # all 1-Hamming matches of w in s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-13T11:58:56.929Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AAA': ['AAC', 'ACA', 'AAA', 'TAA', 'AGA', 'AAG', 'ATA', 'GAA', 'CAA', 'AAT'], 'AAT': ['TAT', 'CAT', 'AAC', 'AGT', 'AAA', 'AAG', 'ATT', 'GAT', 'ACT', 'AAT'], 'AAC': ['ATC', 'AAC', 'GAC', 'TAC', 'AAA', 'CAC', 'AAG', 'ACC', 'AGC', 'AAT'], 'AAG': ['AAC', 'AAA', 'AAG', 'CAG', 'AGG', 'GAG', 'ATG', 'ACG', 'TAG', 'AAT'], 'ATA': ['ATC', 'ACA', 'AAA', 'AGA', 'ATA', 'ATT', 'CTA', 'ATG', 'TTA', 'GTA'], 'ATT': ['ATC', 'AGT', 'GTT', 'ATT', 'ATA', 'ATG', 'ACT', 'CTT', 'TTT', 'AAT'], 'ATC': ['ATC', 'AAC', 'TTC', 'GTC', 'ATA', 'ATT', 'ACC', 'ATG', 'AGC', 'CTC'], 'ATG': ['ATC', 'GTG', 'CTG', 'AAG', 'ATA', 'ATT', 'AGG', 'ATG', 'ACG', 'TTG'], 'ACA': ['CCA', 'ACA', 'AAA', 'GCA', 'AGA', 'ATA', 'TCA', 'ACC', 'ACG', 'ACT'], 'ACT': ['AGT', 'ACA', 'GCT', 'CCT', 'ATT', 'TCT', 'ACC', 'ACG', 'ACT', 'AAT'], 'ACC': ['ATC', 'AAC', 'GCC', 'ACA', 'TCC', 'ACT', 'ACC', 'ACG', 'AGC', 'CCC'], 'ACG': ['TCG', 'GCG', 'ACA', 'CCG', 'AAG', 'AGG', 'ATG', 'ACG', 'ACC', 'ACT'], 'AGA': ['ACA', 'AAA', 'AGA', 'AGT', 'ATA', 'GGA', 'TGA', 'AGG', 'AGC', 'CGA'], 'AGT': ['CGT', 'AGT', 'AGA', 'TGT', 'GGT', 'ATT', 'AGC', 'AGG', 'ACT', 'AAT'], 'AGC': ['ATC', 'AAC', 'AGT', 'AGA', 'CGC', 'GGC', 'AGG', 'ACC', 'AGC', 'TGC'], 'AGG': ['CGG', 'AGT', 'TGG', 'AGA', 'AAG', 'AGG', 'GGG', 'ATG', 'ACG', 'AGC'], 'TAA': ['TAT', 'TAC', 'AAA', 'TAA', 'TCA', 'TGA', 'GAA', 'TTA', 'TAG', 'CAA'], 'TAT': ['TAT', 'CAT', 'TAC', 'TAA', 'TGT', 'GAT', 'TCT', 'TAG', 'TTT', 'AAT'], 'TAC': ['GAC', 'AAC', 'TAT', 'TAC', 'TTC', 'TAA', 'CAC', 'TCC', 'TAG', 'TGC'], 'TAG': ['TCG', 'TAT', 'TAC', 'TGG', 'TAA', 'AAG', 'CAG', 'GAG', 'TAG', 'TTG'], 'TTA': ['TTC', 'TAA', 'ATA', 'TCA', 'TGA', 'CTA', 'TTA', 'GTA', 'TTG', 'TTT'], 'TTT': ['TAT', 'TTC', 'TGT', 'GTT', 'ATT', 'TCT', 'TTA', 'CTT', 'TTG', 'TTT'], 'TTC': ['ATC', 'TAC', 'TTC', 'GTC', 'TCC', 'TTT', 'TTA', 'CTC', 'TTG', 'TGC'], 'TTG': ['TCG', 'GTG', 'CTG', 'TGG', 'TTC', 'ATG', 'TTA', 'TAG', 'TTG', 'TTT'], 'TCA': ['TCG', 'CCA', 'ACA', 'TAA', 'GCA', 'TCA', 'TGA', 'TCC', 'TCT', 'TTA'], 'TCT': ['TAT', 'TCG', 'GCT', 'TGT', 'CCT', 'TCA', 'TCC', 'TCT', 'ACT', 'TTT'], 'TCC': ['TCG', 'GCC', 'TAC', 'TTC', 'TCC', 'TCA', 'ACC', 'TCT', 'TGC', 'CCC'], 'TCG': ['TCG', 'GCG', 'TGG', 'CCG', 'TCA', 'TCC', 'TCT', 'ACG', 'TAG', 'TTG'], 'TGA': ['AGA', 'TAA', 'TGG', 'TGT', 'GGA', 'TGA', 'TCA', 'TTA', 'CGA', 'TGC'], 'TGT': ['TAT', 'CGT', 'AGT', 'TGG', 'TGT', 'GGT', 'TGC', 'TGA', 'TCT', 'TTT'], 'TGC': ['TAC', 'TTC', 'TGG', 'CGC', 'TGT', 'GGC', 'TCC', 'TGA', 'AGC', 'TGC'], 'TGG': ['CGG', 'TCG', 'TGG', 'TGT', 'TGA', 'AGG', 'GGG', 'TAG', 'TTG', 'TGC'], 'CAA': ['CAT', 'CCA', 'AAA', 'TAA', 'CAC', 'CAG', 'CTA', 'GAA', 'CGA', 'CAA'], 'CAT': ['TAT', 'CAT', 'CGT', 'CAC', 'CAG', 'GAT', 'CCT', 'CTT', 'CAA', 'AAT'], 'CAC': ['GAC', 'AAC', 'CAT', 'TAC', 'CAC', 'CGC', 'CAG', 'CTC', 'CAA', 'CCC'], 'CAG': ['CGG', 'CAT', 'CTG', 'CCG', 'CAC', 'AAG', 'CAG', 'GAG', 'TAG', 'CAA'], 'CTA': ['CCA', 'CTT', 'CTG', 'ATA', 'CTA', 'CTC', 'TTA', 'CGA', 'GTA', 'CAA'], 'CTT': ['CAT', 'CGT', 'CTG', 'GTT', 'ATT', 'CTA', 'CCT', 'CTT', 'CTC', 'TTT'], 'CTC': ['ATC', 'TTC', 'CTG', 'CAC', 'CGC', 'GTC', 'CTA', 'CTT', 'CTC', 'CCC'], 'CTG': ['CGG', 'GTG', 'CTG', 'CCG', 'CAG', 'CTA', 'ATG', 'CTT', 'CTC', 'TTG'], 'CCA': ['CCA', 'ACA', 'GCA', 'CCG', 'TCA', 'CTA', 'CCT', 'CGA', 'CAA', 'CCC'], 'CCT': ['CAT', 'CCA', 'CGT', 'GCT', 'CCG', 'CCT', 'TCT', 'ACT', 'CTT', 'CCC'], 'CCC': ['GCC', 'CCA', 'CAC', 'CGC', 'CCG', 'TCC', 'ACC', 'CCT', 'CTC', 'CCC'], 'CCG': ['TCG', 'CGG', 'CCA', 'GCG', 'CTG', 'CCG', 'CAG', 'ACG', 'CCT', 'CCC'], 'CGA': ['CGG', 'CCA', 'CGT', 'AGA', 'CGC', 'GGA', 'TGA', 'CTA', 'CGA', 'CAA'], 'CGT': ['CAT', 'CGG', 'CGT', 'AGT', 'CGC', 'TGT', 'GGT', 'CGA', 'CCT', 'CTT'], 'CGC': ['CGG', 'CGT', 'CAC', 'CGC', 'GGC', 'AGC', 'CGA', 'CTC', 'TGC', 'CCC'], 'CGG': ['CGG', 'CGT', 'CTG', 'TGG', 'CCG', 'CGC', 'CAG', 'AGG', 'GGG', 'CGA'], 'GAA': ['GAC', 'AAA', 'TAA', 'GCA', 'GGA', 'GAG', 'GAT', 'GAA', 'GTA', 'CAA'], 'GAT': ['TAT', 'CAT', 'GAC', 'GCT', 'GGT', 'GTT', 'GAG', 'GAT', 'GAA', 'AAT'], 'GAC': ['GAC', 'AAC', 'GCC', 'TAC', 'CAC', 'GTC', 'GGC', 'GAG', 'GAT', 'GAA'], 'GAG': ['GAC', 'GTG', 'GCG', 'AAG', 'CAG', 'GAG', 'GGG', 'GAT', 'GAA', 'TAG'], 'GTA': ['GTG', 'GCA', 'GTC', 'ATA', 'GGA', 'GTT', 'CTA', 'GAA', 'TTA', 'GTA'], 'GTT': ['GTG', 'GCT', 'GGT', 'GTT', 'ATT', 'GTC', 'GAT', 'GTA', 'CTT', 'TTT'], 'GTC': ['ATC', 'GAC', 'GCC', 'GTG', 'TTC', 'GTC', 'GGC', 'GTT', 'GTA', 'CTC'], 'GTG': ['GTG', 'GCG', 'CTG', 'GTC', 'GTT', 'GAG', 'GGG', 'ATG', 'GTA', 'TTG'], 'GCA': ['CCA', 'GCC', 'GCG', 'ACA', 'GCA', 'GCT', 'TCA', 'GGA', 'GAA', 'GTA'], 'GCT': ['GCC', 'GCG', 'GCT', 'GCA', 'CCT', 'GTT', 'GGT', 'GAT', 'TCT', 'ACT'], 'GCC': ['GAC', 'GCC', 'GCG', 'GCA', 'GCT', 'GTC', 'GGC', 'TCC', 'ACC', 'CCC'], 'GCG': ['TCG', 'GTG', 'GCC', 'GCG', 'GCA', 'CCG', 'GCT', 'GAG', 'GGG', 'ACG'], 'GGA': ['AGA', 'GCA', 'GGT', 'GGC', 'GGA', 'TGA', 'GGG', 'GAA', 'GTA', 'CGA'], 'GGT': ['CGT', 'AGT', 'GCT', 'TGT', 'GGT', 'GTT', 'GGA', 'GGC', 'GGG', 'GAT'], 'GGC': ['GAC', 'GCC', 'CGC', 'GGC', 'GTC', 'GGA', 'GGT', 'GGG', 'AGC', 'TGC'], 'GGG': ['CGG', 'GTG', 'GCG', 'TGG', 'GGT', 'GGC', 'GGA', 'AGG', 'GGG', 'GAG']}\n"
     ]
    }
   ],
   "source": [
    "voc_neigh = voc.copy()\n",
    "for w in voc_neigh:\n",
    "    voc_neigh[w] = build_1_neighborhood(w)\n",
    "print(voc_neigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-13T11:58:56.930Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_1_hamming_embedding(x, length, voc_neigh):\n",
    "    st = STree.STree(x)\n",
    "    enc = np.zeros(len(voc_neigh))\n",
    "    i = 0\n",
    "    for w in voc_neigh.keys():\n",
    "        nset = voc_neigh[w]\n",
    "        enc[i] = sum([len(st.find_all(n)) for n in nset]) # all 1-Hamming matches of w in s0\n",
    "        i+=1\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-13T11:58:56.932Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.,  5., 12.,  9.,  2.,  3., 10., 16., 11., 15., 20., 15.,  7.,\n",
       "       10., 21., 14.,  6.,  3., 11.,  8.,  4.,  6., 10., 16., 10., 13.,\n",
       "       27., 14., 12., 14., 24., 17., 13., 10., 18., 22., 16., 14., 24.,\n",
       "       19., 23., 24., 36., 32., 13., 13., 27., 29.,  6., 12., 17., 12.,\n",
       "        6., 13., 20., 16., 20., 22., 35., 22., 15., 18., 27., 24.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters = 'ATCG'\n",
    "length = 3\n",
    "voc = build_voc(letters, length)\n",
    "voc_neigh = voc.copy()\n",
    "for w in voc_neigh:\n",
    "    voc_neigh[w] = build_1_neighborhood(w)\n",
    "\n",
    "compute_1_hamming_embedding(s0, length, voc_neigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-13T11:58:56.933Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  3.,  2.,  3.,  1.,  1.,  3.,  3.,  0.,  2.,  7.,  3.,  2.,\n",
       "        4.,  4.,  5.,  0.,  0.,  1.,  1.,  1.,  2.,  2.,  1.,  1.,  2.,\n",
       "        6.,  1.,  1.,  2.,  8.,  5.,  2.,  1.,  4.,  4.,  0.,  2.,  4.,\n",
       "        9.,  8.,  6., 10.,  7.,  1.,  2.,  8.,  4.,  1.,  1.,  4.,  1.,\n",
       "        0.,  3.,  5.,  2.,  2.,  8., 10.,  4.,  3.,  5.,  9.,  6.,  2.,\n",
       "        1.,  1.,  2.,  0.,  1.,  1.,  1.,  1.,  2.,  5.,  3.,  0.,  2.,\n",
       "        3.,  3.,  2.,  0.,  2.,  0.,  1.,  0.,  4.,  1.,  2.,  5.,  4.,\n",
       "        5.,  2.,  3.,  7.,  4.,  2.,  2.,  4.,  5.,  2.,  2.,  3.,  9.,\n",
       "        8.,  5., 11.,  8.,  2.,  3.,  5.,  4.,  3.,  4.,  5.,  3.,  1.,\n",
       "        5.,  5.,  6.,  6., 11., 14.,  5.,  4.,  6., 11.,  8.,  4.,  2.,\n",
       "        6.,  7.,  1.,  2.,  2.,  5.,  6.,  7.,  6.,  5.,  4.,  4., 10.,\n",
       "       10.,  3.,  3.,  5.,  5.,  0.,  2.,  5.,  7.,  5.,  5., 11.,  6.,\n",
       "       10.,  9., 11., 13., 11.,  6.,  8., 10.,  8.,  4., 10., 11., 10.,\n",
       "       12., 19., 15.,  8.,  6., 12., 18.,  3.,  4.,  5.,  4.,  1.,  3.,\n",
       "        2.,  7.,  8.,  8., 12.,  8.,  4.,  8., 14.,  9.,  2.,  2.,  2.,\n",
       "        3.,  0.,  2.,  3.,  7.,  3.,  3., 10.,  3.,  1.,  0.,  6.,  5.,\n",
       "        1.,  0.,  3.,  2.,  2.,  2.,  5.,  7.,  5.,  4., 11.,  2.,  2.,\n",
       "        3., 10.,  7.,  4.,  1.,  8.,  9.,  6.,  6., 10., 14.,  9., 10.,\n",
       "       16., 14.,  5.,  2., 11.,  9.,  2.,  3.,  6.,  4.,  4.,  5., 11.,\n",
       "        7.,  7., 10., 16., 11.,  4.,  8., 11.,  7.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters = 'ATCG'\n",
    "length = 4\n",
    "voc = build_voc(letters, length)\n",
    "voc_neigh = voc.copy()\n",
    "for w in voc_neigh:\n",
    "    voc_neigh[w] = build_1_neighborhood(w)\n",
    "\n",
    "compute_1_hamming_embedding(s0, length, voc_neigh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the data matrices (Hamming 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-13T11:58:56.935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has size 64\n"
     ]
    }
   ],
   "source": [
    "letters = 'ATCG'\n",
    "length = 3\n",
    "voc = build_voc(letters, length)\n",
    "voc_neigh = voc.copy()\n",
    "for w in voc_neigh:\n",
    "    voc_neigh[w] = build_1_neighborhood(w)\n",
    "    \n",
    "print('Vocabulary has size', len(voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-13T11:58:56.937Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:18, 107.51it/s]\n",
      "2000it [00:18, 110.76it/s]\n",
      "1149it [00:10, 79.05it/s] "
     ]
    }
   ],
   "source": [
    "for ind in range(3):\n",
    "    df = pd.read_csv('kernel-methods-for-machine-learning-2018-2019/Xtr'+str(ind)+'.csv')\n",
    "    df_emb = pd.DataFrame(columns = [str(i) for i in range(len(voc))])\n",
    "    for _, r in tqdm.tqdm(df.iterrows()):\n",
    "        i = r['Id']\n",
    "        seq = r['seq']\n",
    "        enc = compute_1_hamming_embedding(seq, length, voc_neigh)\n",
    "        df_emb.loc[i] = enc\n",
    "    df_emb.to_csv('kernel-methods-for-machine-learning-2018-2019/'\n",
    "              + 'Xtr' +str(ind) + '_spectr'+str(length)+'_hamming1'+'.csv', header = False, index = False, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-13T11:58:56.939Z"
    }
   },
   "outputs": [],
   "source": [
    "for ind in range(3):\n",
    "    df = pd.read_csv('kernel-methods-for-machine-learning-2018-2019/Xte'+str(ind)+'.csv')\n",
    "    df_emb = pd.DataFrame(columns = [str(i) for i in range(len(voc))])\n",
    "    for _, r in tqdm.tqdm(df.iterrows()):\n",
    "        i = r['Id']\n",
    "        seq = r['seq']\n",
    "        enc = compute_1_hamming_embedding(seq, length, voc_neigh)\n",
    "        df_emb.loc[i] = enc\n",
    "    df_emb.to_csv('kernel-methods-for-machine-learning-2018-2019/'\n",
    "              + 'Xte' +str(ind) + '_spectr'+str(length)+'_hamming1'+'.csv', header = False, index = False, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:optim]",
   "language": "python",
   "name": "conda-env-optim-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
