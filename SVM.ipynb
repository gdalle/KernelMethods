{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel methods for biological sequence classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MVA 2019 - Kernel methods for machine learning\n",
    "\n",
    "*Éloïse Berthier, Guillaume Dalle, Clément Mantoux*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T12:30:28.689938Z",
     "start_time": "2019-02-13T12:30:26.674475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-13 13:30:27,802\tWARNING worker.py:1354 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n",
      "2019-02-13 13:30:27,804\tINFO node.py:278 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-02-13_13-30-27_1511/logs.\n",
      "2019-02-13 13:30:27,911\tINFO services.py:396 -- Waiting for redis server at 127.0.0.1:53950 to respond...\n",
      "2019-02-13 13:30:28,041\tINFO services.py:396 -- Waiting for redis server at 127.0.0.1:17694 to respond...\n",
      "2019-02-13 13:30:28,049\tINFO services.py:798 -- Starting Redis shard with 10.0 GB max memory.\n",
      "2019-02-13 13:30:28,076\tINFO services.py:1360 -- Starting the Plasma object store with 6.871947672999999 GB memory using /tmp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8889/notebooks/ray_ui.ipynb?token=d8e8c17128e92b20e9b7287ab7b7f3d404e5e2e01acbafe5\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cProfile, pstats\n",
    "import tqdm\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "\n",
    "import cvxpy as cp\n",
    "import cvxopt\n",
    "from qpsolvers import solve_qp\n",
    "import osqp\n",
    "\n",
    "import ray\n",
    "ray.init()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from vector_kernels import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T12:30:28.699598Z",
     "start_time": "2019-02-13T12:30:28.692634Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data_mat100(dataset=\"tr0\"):\n",
    "    folder = \"kernel-methods-for-machine-learning-2018-2019\"\n",
    "    features_file = \"X\" + dataset + \"_mat100.csv\"\n",
    "    labels_file = \"Y\" + dataset + \".csv\"\n",
    "    \n",
    "    X = pd.read_csv(\n",
    "        os.path.join(folder, features_file),\n",
    "        sep=\" \",\n",
    "        header=None\n",
    "    )\n",
    "    if \"te\" in dataset:\n",
    "        return np.array(X)\n",
    "    \n",
    "    elif \"tr\" in dataset:\n",
    "        Y = pd.read_csv(\n",
    "            os.path.join(folder, labels_file),\n",
    "            sep=\",\",\n",
    "            index_col=0,\n",
    "        )\n",
    "        return np.array(X), 2 * np.array(Y.iloc[:, 0]) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T12:30:28.712083Z",
     "start_time": "2019-02-13T12:30:28.701797Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data_spectr(dataset=\"tr0\", length = 3):\n",
    "    folder = \"kernel-methods-for-machine-learning-2018-2019/\"\n",
    "    features_file = \"X\" + dataset + '_spectr'+str(length)+'.csv'\n",
    "    labels_file = \"Y\" + dataset + \".csv\"\n",
    "    \n",
    "    X = pd.read_csv(\n",
    "        os.path.join(folder, features_file),\n",
    "        sep=\" \",\n",
    "        header=None\n",
    "    )\n",
    "    if \"te\" in dataset:\n",
    "        return np.array(X)\n",
    "    \n",
    "    elif \"tr\" in dataset:\n",
    "        Y = pd.read_csv(\n",
    "            os.path.join(folder, labels_file),\n",
    "            sep=\",\",\n",
    "            index_col=0,\n",
    "        )\n",
    "        return np.array(X), 2 * np.array(Y.iloc[:, 0]) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T12:30:30.358790Z",
     "start_time": "2019-02-13T12:30:28.714785Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for k in [0, 1, 2]:\n",
    "    #Xtr, Ytr = read_data_mat100(\"tr\" + str(k))\n",
    "    #Xte = read_data_mat100(\"te\" + str(k))\n",
    "    Xtr, Ytr = read_data_spectr(\"tr\" + str(k), length=4)\n",
    "    Xte = read_data_spectr(\"te\" + str(k), length=4)\n",
    "    dataset.append([Xtr, Ytr, Xte])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM backend & Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadratic Program optimization for kernel SVM with ridge penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T12:30:30.609500Z",
     "start_time": "2019-02-13T12:30:30.601417Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_predictor(Xtr, Ytr, kernel, lambd, method=\"qpsolvers\"):\n",
    "    m = Xtr.mean(axis=0)\n",
    "    s = Xtr.std(axis=0)\n",
    "    Xc = (Xtr - m)/s\n",
    "\n",
    "    n = len(Xc)\n",
    "\n",
    "    I = np.eye(n)\n",
    "    gram_matrix = kernel(Xc, Xc)\n",
    "    K = gram_matrix + 1e-9*I\n",
    "    \n",
    "    if method == \"cvxpy\":\n",
    "    \n",
    "        alpha = cp.Variable(n)\n",
    "\n",
    "        constraints = [\n",
    "            cp.multiply(Ytr, alpha) >= np.zeros(n),\n",
    "            cp.multiply(Ytr, alpha) <= np.ones(n) / (2 * lambd * n)\n",
    "        ]\n",
    "\n",
    "        objective = cp.Minimize(\n",
    "            - 2 * (Ytr * alpha)\n",
    "            + cp.quad_form(alpha, K)\n",
    "        )\n",
    "\n",
    "        prob = cp.Problem(objective, constraints)\n",
    "        result = prob.solve(solver=cp.OSQP, verbose=False)\n",
    "        alpha_opt = alpha.value\n",
    "        \n",
    "    elif method == \"qpsolvers\":\n",
    "        \n",
    "        P = K\n",
    "        q = - Ytr.astype(float)\n",
    "        # Sparse G\n",
    "        G = sp.vstack([\n",
    "            -sp.diags(Ytr),\n",
    "            sp.diags(Ytr)\n",
    "        ]).tocsc().astype(float)\n",
    "        h = np.hstack([\n",
    "            np.zeros(n),\n",
    "            np.ones(n) / (2 * lambd * n)\n",
    "        ]).astype(float)\n",
    "    \n",
    "        alpha_opt = solve_qp(P=P, q=q, G=G, h=h, solver=\"cvxopt\")\n",
    "    \n",
    "    return lambda x_new: np.sign(alpha_opt.dot(kernel(Xc, (x_new - m)/s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profiling both optimization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T12:30:32.272843Z",
     "start_time": "2019-02-13T12:30:31.752057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 13 13:30:32 2019    profiling/stats2\n",
      "\n",
      "         2848 function calls (2828 primitive calls) in 0.498 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 277 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       16    0.253    0.016    0.253    0.016 {built-in method cvxopt.lapack.potrf}\n",
      "        1    0.052    0.052    0.052    0.052 {method 'dot' of 'numpy.ndarray' objects}\n",
      "        1    0.047    0.047    0.102    0.102 vector_kernels.py:9(kernel)\n",
      "        8    0.036    0.004    0.297    0.037 misc.py:1389(factor)\n",
      "        4    0.023    0.006    0.023    0.006 cvxopt_.py:29(cvxopt_matrix)\n",
      "        1    0.018    0.018    0.494    0.494 <ipython-input-5-cdfb343cdacc>:1(compute_predictor)\n",
      "       30    0.016    0.001    0.016    0.001 {built-in method cvxopt.blas.trsv}\n",
      "        8    0.008    0.001    0.008    0.001 {built-in method cvxopt.base.syrk}\n",
      "        1    0.008    0.008    0.008    0.008 twodim_base.py:140(eye)\n",
      "        1    0.008    0.008    0.008    0.008 _methods.py:91(_var)\n",
      "        8    0.006    0.001    0.006    0.001 {built-in method cvxopt.base.symv}\n",
      "        1    0.004    0.004    0.498    0.498 <string>:1(<module>)\n",
      "        2    0.003    0.001    0.003    0.001 {built-in method _imp.create_dynamic}\n",
      "       29    0.003    0.000    0.003    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "        1    0.002    0.002    0.331    0.331 coneprog.py:1440(coneqp)\n",
      "        7    0.002    0.000    0.002    0.000 misc.py:422(update_scaling)\n",
      "        2    0.001    0.001    0.002    0.001 linalg.py:2203(norm)\n",
      "        1    0.001    0.001    0.356    0.356 __init__.py:110(solve_qp)\n",
      "        1    0.001    0.001    0.332    0.332 coneprog.py:4158(qp)\n",
      "       92    0.001    0.000    0.001    0.000 {built-in method cvxopt.base.gemv}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x10e859080>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr, Ytr, _ = dataset[0]\n",
    "\n",
    "#cProfile.run(\"f = compute_predictor(Xtr, Ytr, gauss(3), 1, method='cvxpy')\", \"profiling/stats1\")\n",
    "#pstats.Stats(\"profiling/stats1\").strip_dirs().sort_stats(\"tottime\").print_stats(20)\n",
    "\n",
    "cProfile.run(\"f = compute_predictor(Xtr, Ytr, gauss(3), 1, method='qpsolvers')\", \"profiling/stats2\")\n",
    "pstats.Stats(\"profiling/stats2\").strip_dirs().sort_stats(\"tottime\").print_stats(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T12:30:33.647961Z",
     "start_time": "2019-02-13T12:30:33.638560Z"
    }
   },
   "outputs": [],
   "source": [
    "kfold = 5\n",
    "\n",
    "def cross_validate(X, Y, kernel, lambd, shuffle=True, kfold=kfold):\n",
    "    acc_train, acc_val = np.zeros(kfold), np.zeros(kfold)\n",
    "    \n",
    "    # jointly shuffle input datasets X, Y\n",
    "    n = X.shape[0]\n",
    "    if shuffle:\n",
    "        perm = np.random.permutation(n)\n",
    "        X, Y = X[perm], Y[perm]\n",
    "    idx = np.arange(n)\n",
    "    for k in range(kfold):\n",
    "        # split the datasets\n",
    "        val_idx = idx[k::kfold]\n",
    "        train_idx = np.delete(idx, val_idx)\n",
    "        n_train = len(train_idx)\n",
    "        n_val = n - n_train\n",
    "        \n",
    "        X_train = X[train_idx]\n",
    "        Y_train = Y[train_idx]\n",
    "        X_val = X[val_idx]\n",
    "        Y_val = Y[val_idx]\n",
    "        \n",
    "        # fit the predictor\n",
    "        f = compute_predictor(X_train, Y_train, kernel, lambd)\n",
    "\n",
    "        Yte_train = f(X_train).reshape(-1)\n",
    "        Ypred_train = ((Yte_train + 1) / 2).astype(int)\n",
    "\n",
    "        Yte_val = f(X_val).reshape(-1)\n",
    "        Ypred_val = ((Yte_val + 1) / 2).astype(int)\n",
    "        \n",
    "        Y_train = ((Y_train + 1) / 2).astype(int)\n",
    "        Y_val = ((Y_val + 1) / 2).astype(int)\n",
    "        \n",
    "        # compute metrics\n",
    "        acc_train[k] = np.mean(Y_train == Ypred_train)\n",
    "        acc_val[k] = np.mean(Y_val == Ypred_val)\n",
    "    return acc_train, acc_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tool for pretty plots with mean + std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T12:30:34.399745Z",
     "start_time": "2019-02-13T12:30:34.392232Z"
    }
   },
   "outputs": [],
   "source": [
    "def lineplotCI(x, y, low, up, c, log=False):\n",
    "    if log:\n",
    "        plt.xscale(\"log\")\n",
    "    plt.plot(x, y, lw = 2, color = c, alpha = 1)\n",
    "    plt.fill_between(x, low, up, color = c, alpha = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Grid search on lambda for the linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T11:56:42.465920Z",
     "start_time": "2019-02-13T11:56:33.606Z"
    }
   },
   "outputs": [],
   "source": [
    "lambd_range = np.logspace(-1.5, 3, 100)\n",
    "\n",
    "acc_train = np.zeros((3, len(lambd_range), kfold))\n",
    "acc_val = np.zeros((3, len(lambd_range), kfold))\n",
    "\n",
    "for d, data in enumerate(dataset):\n",
    "    Xtr, Ytr, _ = data\n",
    "    for i in tqdm.trange(len(lambd_range), desc=\"Testing lambda for dataset {}\".format(d+1)):\n",
    "        lambd = lambd_range[i]\n",
    "        acc_train[d, i], acc_val[d, i] = cross_validate(\n",
    "            Xtr, Ytr,\n",
    "            linear(), lambd,\n",
    "            shuffle=True, kfold=kfold\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T11:56:42.467956Z",
     "start_time": "2019-02-13T11:56:33.607Z"
    }
   },
   "outputs": [],
   "source": [
    "d = 0\n",
    "plt.figure()\n",
    "lineplotCI(\n",
    "    lambd_range,\n",
    "    acc_train[d].mean(axis=1),\n",
    "    acc_train[d].mean(axis=1) + acc_train[d].std(axis=1),\n",
    "    acc_train[d].mean(axis=1) - acc_train[d].std(axis=1),\n",
    "    c='r',\n",
    "    log=True\n",
    ")\n",
    "lineplotCI(\n",
    "    lambd_range,\n",
    "    acc_val[d].mean(axis=1),\n",
    "    acc_val[d].mean(axis=1) + acc_val[d].std(axis=1),\n",
    "    acc_val[d].mean(axis=1) - acc_val[d].std(axis=1),\n",
    "    c='g',\n",
    "    log=True\n",
    ")\n",
    "plt.legend(['train', 'val'])\n",
    "plt.xlabel('Value of lambda')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Cross-validation on dataset {}'.format(d))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T11:56:42.469909Z",
     "start_time": "2019-02-13T11:56:33.608Z"
    }
   },
   "outputs": [],
   "source": [
    "best_lambd = [\n",
    "    lambd_range[np.argmax(np.mean(acc_val[d], axis=1))]\n",
    "    for d in range(3)\n",
    "]\n",
    "print(best_lambd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Grid search on lambda & sigma for the gaussian kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T12:31:13.070385Z",
     "start_time": "2019-02-13T12:31:13.065349Z"
    }
   },
   "outputs": [],
   "source": [
    "kfold = 3\n",
    "lambd_range = np.logspace(-1.5, 1, 20)\n",
    "sigma_range = np.linspace(3, 10, 20)\n",
    "tuple_range = list(itertools.product(lambd_range, sigma_range))\n",
    "\n",
    "acc_train = np.zeros((3, len(tuple_range), kfold))\n",
    "acc_val = np.zeros((3, len(tuple_range), kfold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T12:31:13.486960Z",
     "start_time": "2019-02-13T12:31:13.467022Z"
    }
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def joint_crossval(d, data):\n",
    "    Xtr, Ytr, _ = data\n",
    "    for i in tqdm.trange(len(tuple_range), desc=\"Testing (lambd, sigma) for dataset {}\".format(d+1)):\n",
    "        lambd, sigma = tuple_range[i]\n",
    "        acc_train[d, i], acc_val[d, i] = cross_validate(\n",
    "            Xtr, Ytr,\n",
    "            gauss(sigma), lambd,\n",
    "            shuffle=True, kfold=kfold\n",
    "        )\n",
    "    print(\"Finished dataset {}\".format(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-13T12:31:14.474Z"
    }
   },
   "outputs": [],
   "source": [
    "tasks = []\n",
    "for d, data in enumerate(dataset):\n",
    "    tasks.append(joint_crossval.remote(d, data))\n",
    "for t in tasks:\n",
    "    ray.get(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-13T12:34:57.104Z"
    }
   },
   "outputs": [],
   "source": [
    "best_tuple = [\n",
    "    tuple_range[np.argmax(np.mean(acc_val[d], axis=1))]\n",
    "    for d in range(3)\n",
    "]\n",
    "best_lambd = [bt[0] for bt in best_tuple]\n",
    "best_sigma = [bt[1] for bt in best_tuple]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-13T12:34:57.537Z"
    }
   },
   "outputs": [],
   "source": [
    "print(np.mean(acc_train[0], axis=1))\n",
    "print(np.mean(acc_train[1], axis=1))\n",
    "print(np.mean(acc_train[2], axis=1))\n",
    "print(np.mean(acc_val[0], axis=1))\n",
    "print(np.mean(acc_val[1], axis=1))\n",
    "print(np.mean(acc_val[2], axis=1))\n",
    "print(tuple_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works quite well with **datasets 2 and 3 (not 1!)** and tends to overfit..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Final predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-13T12:34:59.017Z"
    }
   },
   "outputs": [],
   "source": [
    "Ypred = []\n",
    "lambd = 0.\n",
    "\n",
    "for k in [0, 1, 2]:\n",
    "    print(\"\\nDATASET {}\\n\".format(k))\n",
    "\n",
    "    Xtr, Ytr, Xte = dataset[k]\n",
    "    \n",
    "    f = compute_predictor(Xtr, Ytr, gauss(best_sigma[k]), best_lambd[k])\n",
    "    #f = compute_predictor(Xtr, Ytr, linear(), best_lambd[k])\n",
    "    print(np.mean(Ytr == f(Xtr)))\n",
    "    Yte = f(Xte)\n",
    "    \n",
    "    Ypred.extend(list(((Yte + 1) / 2).astype(int)))\n",
    "    \n",
    "Ypred = pd.Series(\n",
    "    index=np.arange(len(Ypred)),\n",
    "    data=Ypred\n",
    ")\n",
    "Ypred.index.name = \"Id\"\n",
    "Ypred.name = \"Bound\"\n",
    "Ypred.to_csv(\"Ypred.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:optim]",
   "language": "python",
   "name": "conda-env-optim-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
